\documentclass[aps,12pt]{revtex4}
\usepackage{graphicx}
\usepackage{amssymb,amsfonts,amsmath,amsthm}
\usepackage{chemarr}
\usepackage{bm}
\usepackage{pslatex}
\usepackage{mathptmx}
\usepackage{xfrac}
\usepackage{bookman}

%%  
\newcommand{\trn}[1]{{#1}^{\mathtt{T}}}
 
\begin{document}

\section{Notations}

Let us assume that $\vec{F}(\vec{X})$ is derivable at $\vec{X}$.
Since:
\begin{equation}
	\vec{F}(\vec{X}+\delta\vec{X}) \simeq \vec{F}_0 + \bm{J} \delta\vec{X}
\end{equation}
The associated optimisation problem is to minimise:
\begin{equation}
	g(\vec{X}) = \dfrac{1}{2} \vec{F}^2
\end{equation}

\begin{equation}
	g(\vec{X}+\delta\vec{X}) = g_0 + \trn{\vec{\nabla}g} \delta\vec{X} + \dfrac{1}{2} \trn{\delta\vec{X}} \bm{H} \delta\vec{X}
\end{equation}

\begin{equation}
	\vec{\nabla}g_j = \sum_i F_i \dfrac{\partial F_i}{\partial X_j} \; \Leftrightarrow \; \vec{\nabla}g = \trn{\bm{J}} \vec{F}
\end{equation}

\begin{equation}
	\bm{H}_{jk} = \partial_{X_k} \vec{\nabla}g_j 
	= 
	\left( \sum_i F_i \dfrac{\partial^2 F_i}{\partial X_j \partial X_k} \right)
	+ \left( \sum_i \dfrac{\partial F_i}{\partial X_j} \dfrac{\partial F_i}{\partial X_k}\right)
\end{equation}
We define the curvature $\bm{C}$ such that
\begin{equation}
		\bm{C}_{jk} =   \sum_i \dfrac{\partial F_i}{\partial X_j} \dfrac{\partial F_i}{\partial X_k} \; \Leftrightarrow \;
		\bm{C} = \trn{\bm{J}}\bm{J}
\end{equation}

Locally, we want to minimise
\begin{equation}
	\label{eq:min}
	g_0 +  \trn{\vec{\nabla}g} \delta\vec{X} + \dfrac{1}{2} \trn{\delta\vec{X}} \bm{C} \delta\vec{X}
\end{equation}



\section{Regular curvature}
The minimum is given by solving:
\begin{equation}
\bm{C} \delta\vec{X} = -\vec{\nabla}g  \; \Leftrightarrow \;  \delta\vec{X} = - \bm{J}^{-1} \vec{F}
\end{equation}
which is the Newton's step, formally setting \eqref{eq:min} to zero.

\section{Generic case}
We define $\bm{C}_\lambda$ a regularised matrix from $\bm{C}$.
The diagonal terms of $\bm{C}$ are
\begin{equation}
	\forall j, \; d_j = \bm{C}_{jj} = \sum_i \left(\partial_{X_j} F_i\right)^2
\end{equation}
We compute
\begin{equation}
	d'_j = d_j (1+\omega_j \lambda)
\end{equation}
so that:
\begin{equation}
	d'_j \approx d_j \omega_j \lambda
\end{equation}
We compute $\omega_j$ using a pivot value $p>d_{max}$ such that
\begin{equation}
	\omega_j = \dfrac{p-d_j}{\beta}
\end{equation}
 We compute
\begin{equation}
	<\omega> = \dfrac{1}{\beta}\left[p-<d>\right]
\end{equation}
and
\begin{equation}
	<d\omega> = \dfrac{1}{\beta} \left[ p <d> - <d^2>\right]
\end{equation}
We conserve the average coefficients:
\begin{equation}
	\dfrac{1}{\beta} \left[ p <d> - <d^2>\right] = <d>
\end{equation}
 so that
 \begin{equation}
 	\beta = p - \dfrac{<d^2>}{<d>}
 \end{equation}
 
 
\end{document}