\documentclass[aps,12pt]{revtex4}
\usepackage{graphicx}
\usepackage{amssymb,amsfonts,amsmath,amsthm}
\usepackage{chemarr}
\usepackage{bm}
\usepackage{pslatex}
\usepackage{mathptmx}
\usepackage{xfrac}
\usepackage{bookman}
 
\newcommand{\trn}[1]{~^t{#1}}
 
\begin{document}
\title{General Least Squares}
\maketitle

\section{Least Squares}
We assume that we have $S\in \mathbb{N}^*$ sets of \textbf{scalar observable values} depending on $d-$dimensional coordinates:
\begin{equation}
	\forall s \in [1:S], \; \forall i \in [1:N_s], \; Y_{si} \in \mathbb{R} \text{ at } \vec{X}_{si} \in \mathbb{R}^d.
\end{equation}

We want to fit this data with a global set of parameters $\vec{A}\in \mathbb{R}^M$,
with the same function: 
\begin{equation}
F : \mathbb{R}^d \times \mathbb{R}^m \rightarrow \mathbb{R},
\end{equation}
that will depend on a different subset $\vec{A}_s\in\mathbb{R}^m$:
\begin{equation}
	\forall s \in [1:S], \; F(\vec{X},\vec{A}_s), \;\; \vec{A}_s = \bm{\Gamma}_s \vec{A}, \;\; \bm{\Gamma}_s \in \mathcal{M}_{m,M}(\{0,1\}),\;\;m\leq M
\end{equation}
The matrix $\bm{\Gamma}_s$ is the "subset" operator.\\
The least squares of a sample is:
\begin{equation}
		\forall s \in [1:S], \; D^2_s = \sum_{i=1}^{N_s} \left[ Y_{si} - F(\vec{X}_{si},\vec{A}_s) \right]^2
\end{equation}
and the total least square is:
\begin{equation}
	D^2(\vec{A}) = \frac{1}{N}\sum_s N_s D^2_s (\vec{A}_s ),\;\; N=\sum_s N_s
\end{equation}

\section{Descent direction(s)}
Let us compute the local descent direction:
\begin{equation}
	\vec{\beta}_s = - \vec{\nabla}_{\vec{A}_s} D^2_s =  \sum_{i=1}^{N_s} \left[ Y_{si} - F(\vec{X}_{si},\vec{A}_s) \right] \vec{\nabla} F(\vec{X}_{si},\vec{A}_s)
\end{equation}

Let us compute the descent direction:
\begin{equation}
 	\vec{\beta}    = - \vec{\nabla}_{\vec{A}} D^2 
	 =  \dfrac{1}{N} \sum_s N_s \trn{\bm{\Gamma}}_s \vec{\beta}_s
\end{equation}

We want to find $\vec{\beta} = \vec{0}$.

\section{Curvature}
\begin{equation}
	%\partial_{\vec{A}} \vec{\beta} = \dfrac{1}{N} \sum_s N_s \trn{\bm{\Gamma}}_s \partial_{\vec{A}} \vec{\beta}_s
	\partial_{\vec{A}_s} \vec{\beta}_s = \sum_{i=1}^{N_s}  \left[ Y_{si} - F(\vec{X}_{si},\vec{A}_s) \right] \bm{H}_F(\vec{X}_{si},\vec{A}_s)
	- \underbrace{
	\sum_{i=1}^{N_s} \vert\vec{\nabla} F(\vec{X}_{si},\vec{A}_s) \rangle  \langle\vec{\nabla} F(\vec{X}_{si},\vec{A}_s)\vert
	}_{\bm{\alpha}_s(\vec{X}_{si},\vec{A}_s)\in\mathcal{M}_{m} }
\end{equation}
We assume that the first term vanishes around the minimum, so that we define:
\begin{equation}
	\bm{\alpha}(\vec{A}) = \dfrac{1}{N} \sum_s N_s \underbrace{\trn{\bm{\Gamma}_s} \alpha_s (\vec{X}_{si},\vec{A}_s) \bm{\Gamma}_s}_{\in \mathcal{M}_M}
\end{equation}

\section{Looking for descent step}

Starting from $\vec{A}$, we look for $\delta\vec{A}$ such that:
\begin{equation}
	\vec{\beta}(\vec{A}+\delta\vec{A}) = \vec{0} \simeq \vec{\beta}(\vec{A}) - \bm{\alpha}(\vec{A}) \cdot \delta \vec{A}
\end{equation}
We use the Levenberg-Marquardt regularisation approach, since $\bm{\alpha}$ is symmetric with a positive (or zero) diagonal.
We define:
\begin{equation}
\label{start}
\forall u,v \in [1:M]^2, \;\;
	\bm{\alpha}_{\lambda}(\vec{A})\vert_{uv} = 
	\left\lbrace
	\begin{array}{rcl}
	(\bm{\alpha}(\vec{A})\vert_{uv}) & \text{if} & u\not=v\\
	(1+\lambda)(\bm{\alpha}(\vec{A})\vert_{uu})  & \text{if} & u=v\\
	\end{array}
	\right.
\end{equation}
We have to increase $\lambda$ (and recomputing from Eq.\eqref{start}) to find an invertible matrix.
Then we have a guess:
\begin{equation}
	\delta\vec{A}_\lambda = \left[\bm{\alpha}_\lambda(\vec{A})\right]^{-1} \cdot \vec{\beta}(\vec{A})
\end{equation}
We have to increase $\lambda$ (and recomputing from Eq.\eqref{start}) so that:
\begin{equation}
	\sigma = \delta\vec{A}_\lambda \cdot \vec{\beta}(\vec{A}) > 0
\end{equation}
 
\section{Looking for a minimum}
Let us define:
\begin{equation}
	f(\gamma)  =  D^2(\vec{A}+\gamma \delta\vec{A}_\lambda)
\end{equation}
We compute $f(1)=f_1$ (the full step) and we need to compare to $f(0)=f_0$.
\begin{itemize}
\item If $f_1\geq f_0$, we need to increase $\lambda$ and recompute the step from Eq.\eqref{start}.
\item if $f_1<f_0$, then accept step an decrease $\lambda$.
\end{itemize}

\section{Scaling management}

\begin{equation}
	\epsilon \leq \lambda = 10^p < \lambda_{\max}
\end{equation}

\begin{equation}
\left\lbrace
\begin{array}{lcl}
	p_{min}  & =  &-\mathrm{[FLOAT|DOUBLE]\_DIG}\\
	\lambda_{min} & = & 0\\
 	\hline
	p_{max} & = &  \mathrm{[FLOAT|DOUBLE]\_MAX\_10\_EXP}\\
	p>p_{max}& \Rightarrow & \text{singularity or stall}\\
	\hline
	p_{ini} & = & \dfrac{p_{min}}{2}\\
\end{array}
\right.
\end{equation}

\section{Error computation}
The number of degrees of freedom $n$ is equal to the number of points $N=\sum_s N_s$ minus the number of used parameters $M'\leq M$.
\begin{itemize}
\item If $n<0$ then the fit is meaningless.
\item If $n=0$, this is an interpolation, there is no error.
\item If $n>0$, then the variance for the variable $A_j$ is:
\begin{equation}
	\sigma^2_j \simeq \dfrac{D^2(\vec{A})}{N-M'} \max\left(0,\left[\alpha_0(\vec{A})\right]^{-1}_{jj}\right)
\end{equation}
and the standard error is:
\begin{equation}
	\mathrm{err}_j = \sqrt{\dfrac{D^2(\vec{A})}{\left(N-M'\right)^2} \max\left(0,\left[\alpha_0(\vec{A})\right]^{-1}_{jj}\right) }
\end{equation}

\end{itemize}

\section{Application to Algebraic Circle Fit}
We have one sample of $N$ points $\vec{X}_{i\in[1:N]}$ with:
\begin{equation}
	\vec{X}_i = \begin{bmatrix}x_i\\y_i\\\end{bmatrix}
\end{equation}
and we want to match the equation:
\begin{equation}
	x^2+y^2=ax+by+c.
\end{equation}
We define:
\begin{equation}
	\forall i \in [1:N], \;\; z_i = x_i^2+y_i^2,
\end{equation}
and
\begin{equation}
	\vec{A} = \begin{bmatrix} a\\b\\c\\ \end{bmatrix}.
\end{equation}
Then:
\begin{equation}
	\bm{\alpha}_0 = 
	\begin{bmatrix}
	\sum x_i^2 & \sum x_iy_i & \sum x_i\\
	\sum x_iy_i & \sum y_i^2 & \sum y_i\\
	\sum x_i & \sum y_i & N \\
	\end{bmatrix}
\end{equation}
and the algebraic solution is:
\begin{equation}
\bm{\alpha}_0 \vec{A} = \begin{bmatrix}
	\sum z_i x_i\\
	\sum z_i y_i\\
	\sum z_i\\
\end{bmatrix}
\end{equation}
Using:
\begin{equation}
	D^2(\vec{A}) = \sum_i \left[z_i - (ax_i+by_i+c)\right]^2, \;\; M'=3,
\end{equation}
we directly find:
\begin{equation}
	\vec{A} = \bm{\alpha}_0^{-1} \begin{bmatrix}
	\sum z_i x_i\\
	\sum z_i y_i\\
	\sum z_i\\
\end{bmatrix}
+ \delta \vec{A}
\end{equation}
and we get:
\begin{equation}
\left\lbrace
\begin{array}{rcl}
	x_c & = & \frac{a}{2} + \frac{\delta a}{2}\\
	y_c & = & \frac{b}{2} + \frac{\delta b}{2}\\
	r   & = & \sqrt{c+x_c^2+y_c^2} + ...\\
\end{array}
\right.
\end{equation}
\end{document}
