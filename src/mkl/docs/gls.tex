\documentclass[aps,12pt]{revtex4}
\usepackage{graphicx}
\usepackage{amssymb,amsfonts,amsmath,amsthm}
\usepackage{chemarr}
\usepackage{bm}
\usepackage{pslatex}
\usepackage{mathptmx}
\usepackage{xfrac}
\usepackage{bookman}
 
\newcommand{\trn}[1]{~^t{#1}}
 
\begin{document}
\title{General Least Squares}
\maketitle

\section{Least Squares}
We assume that we have $S\in \mathbb{N}^*$ sets of \textbf{scalar observable values} depending on $d-$dimensional coordinates:
\begin{equation}
	\forall s \in [1:S], \; \forall i \in [1:N_s], \; Y_{si} \in \mathbb{R} \text{ at } \vec{X}_{si} \in \mathbb{R}^d.
\end{equation}

We want to fit this data with a global set of parameters $\vec{A}\in \mathbb{R}^M$,
with the same function: 
\begin{equation}
F : \mathbb{R}^d \times \mathbb{R}^m \rightarrow \mathbb{R},
\end{equation}
that will depend on a different subset $\vec{A}_s\in\mathbb{R}^m$:
\begin{equation}
	\forall s \in [1:S], \; F(\vec{X},\vec{A}_s), \;\; \vec{A}_s = \bm{\Gamma}_s \vec{A}, \;\; \bm{\Gamma}_s \in \mathcal{M}_{m,M}(\{0,1\}),\;\;m\leq M
\end{equation}
The matrix $\bm{\Gamma}_s$ is the "subset" operator.\\
The least squares of a sample is:
\begin{equation}
		\forall s \in [1:S], \; D^2_s = \sum_{i=1}^{N_s} \left[ Y_{si} - F(\vec{X}_{si},\vec{A}_s) \right]^2
\end{equation}
and the total least square is:
\begin{equation}
	D^2(\vec{A}) = \frac{1}{N}\sum_s N_s D^2_s (\vec{A}_s ),\;\; N=\sum_s N_s
\end{equation}

\section{Descent direction(s)}
Let us compute the local descent direction:
\begin{equation}
	\vec{\beta}_s = - \vec{\nabla}_{\vec{A}_s} D^2_s =  \sum_{i=1}^{N_s} \left[ Y_{si} - F(\vec{X}_{si},\vec{A}_s) \right] \vec{\nabla} F(\vec{X}_{si},\vec{A}_s)
\end{equation}

Let us compute the descent direction:
\begin{equation}
 	\vec{\beta}    = - \vec{\nabla}_{\vec{A}} D^2 
	 =  \dfrac{1}{N} \sum_s N_s \trn{\bm{\Gamma}}_s \vec{\beta}_s
\end{equation}

We want to find $\vec{\beta} = \vec{0}$.

\section{Curvature}
\begin{equation}
	%\partial_{\vec{A}} \vec{\beta} = \dfrac{1}{N} \sum_s N_s \trn{\bm{\Gamma}}_s \partial_{\vec{A}} \vec{\beta}_s
	\partial_{\vec{A}_s} \vec{\beta}_s = \sum_{i=1}^{N_s}  \left[ Y_{si} - F(\vec{X}_{si},\vec{A}_s) \right] \bm{H}_F(\vec{X}_{si},\vec{A}_s)
	- \underbrace{
	\sum_{i=1}^{N_s} \vert\vec{\nabla} F(\vec{X}_{si},\vec{A}_s) \rangle  \langle\vec{\nabla} F(\vec{X}_{si},\vec{A}_s)\vert
	}_{\bm{\alpha}_s(\vec{X}_{si},\vec{A}_s)\in\mathcal{M}_{m} }
\end{equation}
We assume that the first term vanishes around the minimum, so that we define:
\begin{equation}
	\bm{\alpha}(\vec{A}) = \dfrac{1}{N} \sum_s N_s \underbrace{\trn{\bm{\Gamma}_s} \alpha_s (\vec{X}_{si},\vec{A}_s) \bm{\Gamma}_s}_{\in \mathcal{M}_M}
\end{equation}

\section{Looking for descent step}

Starting from $\vec{A}$, we look for $\delta\vec{A}$ such that:
\begin{equation}
	\vec{\beta}(\vec{A}+\delta\vec{A}) = \vec{0} \simeq \vec{\beta}(\vec{A}) - \bm{\alpha}(\vec{A}) \cdot \delta \vec{A}
\end{equation}
We use the Levenberg-Marquardt regularisation approach, since $\bm{\alpha}$ is symmetric with a positive (or zero) diagonal.
We define:
\begin{equation}
\label{start}
\forall u,v \in [1:M]^2, \;\;
	\bm{\alpha}_{\lambda}(\vec{A})\vert_{uv} = 
	\left\lbrace
	\begin{array}{rcl}
	(\bm{\alpha}(\vec{A})\vert_{uv}) & \text{if} & u\not=v\\
	(1+\lambda)(\bm{\alpha}(\vec{A})\vert_{uu})  & \text{if} & u=v\\
	\end{array}
	\right.
\end{equation}
We have to increase $\lambda$ (and recomputing from Eq.\eqref{start}) to find an invertible matrix.
Then we have a guess:
\begin{equation}
	\delta\vec{A}_\lambda = \left[\bm{\alpha}_\lambda(\vec{A})\right]^{-1} \cdot \vec{\beta}(\vec{A})
\end{equation}
We have to increase $\lambda$ (and recomputing from Eq.\eqref{start}) so that:
\begin{equation}
	\sigma = 2\delta\vec{A}_\lambda \cdot \vec{\beta}(\vec{A}) > 0
\end{equation}
 
\section{Looking for a minimum}
Let us define:
\begin{equation}
	f(\gamma)  =  D^2(\vec{A}+\gamma \delta\vec{A}_\lambda)
\end{equation}
We compute $f(1)=f_1$ (the full step) and we need to compare to $f(0)=f_0$.
We should have:
\begin{equation}
	f(\gamma) \approx f_0 - \gamma \sigma %+ \gamma^2 \left(\sigma+f_1-f_0\right)
\end{equation}

\begin{itemize}
\item If $f_1>f_0$, we need to increase $\lambda$ and recompute the step from Eq.\eqref{start}. This is the "backtracking" choice.
In that case, 
\begin{equation}
		f(\gamma) \approx f_0 - \gamma \sigma + \gamma^2 \left(\sigma+f_1-f_0\right)
\end{equation}

\item if $f_1\leq f_0$, then accept step and decrease $\lambda$.
The parabola that starts at $0$ with value $f_0$ and slope $-\sigma$ is:
\begin{equation}
	g(\gamma) = \dfrac{\sigma^2}{4f_0}\left[\gamma - \dfrac{2f_0}{\sigma}\right]^2
\end{equation}
\end{itemize}

\section{Scaling management}

\begin{equation}
	\epsilon \leq \lambda = 10^p < \lambda_{\max}
\end{equation}

\begin{equation}
\left\lbrace
\begin{array}{lcl}
	p_{min}  & =  &-\mathrm{[FLOAT|DOUBLE]\_DIG}\\
	\lambda_{min} & = & 0\\
 	\hline
	p_{max} & = &  \mathrm{[FLOAT|DOUBLE]\_MAX\_10\_EXP}\\
	p>p_{max}& \Rightarrow & \text{singularity or stall}\\
	\hline
	p_{ini} & = & \dfrac{p_{min}}{2}\\
\end{array}
\right.
\end{equation}

\section{Error computation}
The number of degrees of freedom $n$ is equal to the number of points $N=\sum_s N_s$ minus the number of used parameters $M'\leq M$.
\begin{itemize}
\item If $n<0$ then the fit is meaningless.
\item If $n=0$, this is an interpolation, there is no error.
\item If $n>0$, then the variance for the variable $A_j$ is:
\begin{equation}
	\sigma^2_j \simeq \dfrac{D^2(\vec{A})}{N-M'} \max\left(0,\left[\alpha_0(\vec{A})\right]^{-1}_{jj}\right)
\end{equation}
and the standard error is:
\begin{equation}
	\mathrm{err}_j = \sqrt{\dfrac{D^2(\vec{A})}{\left(N-M'\right)^2} \max\left(0,\left[\alpha_0(\vec{A})\right]^{-1}_{jj}\right) }
\end{equation}

\end{itemize}

\section{Circle Fit}
\subsection{Algebraic}
We have one sample of $N$ points $\vec{X}_{i\in[1:N]}$ with:
\begin{equation}
	\vec{X}_i = \begin{bmatrix}x_i\\y_i\\\end{bmatrix}
\end{equation}
and we want to match the equation:
\begin{equation}
	x^2+y^2=ax+by+c.
\end{equation}
We define:
\begin{equation}
	\forall i \in [1:N], \;\; z_i = x_i^2+y_i^2,
\end{equation}
and
\begin{equation}
	\vec{A} = \begin{bmatrix} a\\b\\c\\ \end{bmatrix}.
\end{equation}
Then the algebraic solution is:
\begin{equation}
\label{eq:algebraic_circle}
\begin{bmatrix}
	\sum x_i^2 & \sum x_iy_i & \sum x_i\\
	\sum x_iy_i & \sum y_i^2 & \sum y_i\\
	\sum x_i & \sum y_i & N \\
	\end{bmatrix}
\vec{A} = \begin{bmatrix}
	\sum z_i x_i\\
	\sum z_i y_i\\
	\sum z_i\\
\end{bmatrix}
\end{equation}
The system \eqref{eq:algebraic_circle} may be solved using floating point or rational coordinates.
Then we deduce 
\begin{equation}
\left\lbrace
\begin{array}{rcl}
	x_c & = & \frac{a}{2}  \\
	y_c & = & \frac{b}{2} \\
	r   & = & \sqrt{c+x_c^2+y_c^2}\\
\end{array}
\right.
\end{equation}
\subsection{Euclidean}
We use the set of equations:
\begin{equation}
\left\lbrace
\begin{array}{rcl}
\vec{X}_i & = &\begin{bmatrix} x_i\\ y_i \\ \end{bmatrix}\\
 Y_i & = & 0\\
 F(\vec{X}_i) & = & \sqrt{ (x_i-x_c)^2 + (y_i-y_c)^2} - r\\
 \\
 \hline
 \\
 \partial_{x_c} F & = & \dfrac{x_c-x_i}{\sqrt{ (x_i-x_c)^2 + (y_i-y_c)^2}}\\
 \\
  \partial_{y_c} F & = & \dfrac{x_c-x_i}{\sqrt{ (x_i-x_c)^2 + (y_i-y_c)^2}}\\
  \\
 \partial_{r} F & = & -1\\
\end{array}
\right.
\end{equation}

\section{Conic Fit}
\subsection{Algebraic}
\subsubsection{Coefficients}
We have one sample of $N$ points $\vec{X}_{i\in[1:N]}$ with:
\begin{equation}
	\vec{X}_i = \begin{bmatrix}x_i\\y_i\\\end{bmatrix}
\end{equation}
and we want to match the equation:
\begin{equation}
	ax^2+bxy+cy^2+dx+ey+f=0
\end{equation}
with some constraint on the coefficients to ensure uniqueness.
We set
\begin{equation}
	\vec{Z}_i = \begin{bmatrix}
	x_i^2\\
	x_iy_i\\
	y_i^2\\
	x_i\\
	y_i\\
	1\\
	\end{bmatrix}
	, \;\;
	\vec{A} =
	\begin{bmatrix}
	a\\
	b\\
	c\\
	d\\
	e\\
	f\\
	\end{bmatrix},\;\;
	\bm{C} \in \mathcal{M}_6,
\end{equation}
and we want to minimize
\begin{equation}
\left\lbrace
\begin{array}{rcl}
	\mathcal{L}(\vec{A},\lambda) & = & \sum_i (\vec{A}\cdot\vec{Z}_i)^2 - \lambda \left(\vec{A} \bm{C} \vec{A}-1\right)\\
	\\
	 & = & \displaystyle \vec{A} \cdot \underbrace{\left( \sum_i \vec{Z}_i \otimes \vec{Z}_i \right)}_{\bm{S}} \cdot \vec{A} - \lambda \left(\vec{A} \bm{C} \vec{A}-1\right)\\
\end{array}
\right.
\end{equation}
We compute the gradient of $\mathcal{L}$:
\begin{equation}
\left\lbrace
\begin{array}{rcl}
\partial_\lambda \mathcal{L} & = & 1 - \vec{A} \bm{C} \vec{A}\\
\\
\partial_{\vec{A}} \mathcal{L} & = & \bm{S} \vec{A} - \lambda \bm{C}\vec{A}\\
\end{array}
\right.
\end{equation}
We end up with:
\begin{equation}
	\underbrace{\bm{S}^{-1} \bm{C}}_{\bm{W}} \vec{A} = \dfrac{1}{\lambda} \vec{A}
\end{equation}
Let $\vec{U}_\mu\not=\vec{0}$ be an eigenvector of $\bm{W}$ and $\mu$ its eigenvalue.
We need
\begin{itemize}
\item $\mu\not=0$
\item $\vec{U}_\mu\cdot\bm{C}\cdot\vec{U}_\mu>0$
\end{itemize}
and we have a guess
\begin{equation}
\label{eq:Amu}
	\vec{A}_\mu = \dfrac{ \vec{U}_\mu}{\sqrt{\vec{U}_\mu\cdot\bm{C}\cdot\vec{U}_\mu}} \Rightarrow \vec{A}_\mu \bm{C} \vec{A}_\mu-1 = 0.
\end{equation}
We evaluate:
\begin{equation}
\mathcal{L}(\vec{A}_\mu,\lambda) = \vec{A}_\mu \cdot \bm{S} \cdot \vec{A}_\mu 
= \vec{A}_\mu \cdot \bm{S} \left( \lambda \bm{S}^{-1} \bm{C} \vec{A}_\mu \right) = \lambda = \dfrac{1}{\mu}
\end{equation}

To minimize $\mathcal{L}$, we need to take the \textbf{highest positive $\mu$} such that its eigenvector $\vec{U}_\mu$ verifies
 $\vec{U}_\mu\cdot\bm{C}\cdot\vec{U}_\mu>0$, and deduce $\vec{A}_\mu$ from \eqref{eq:Amu}.

\subsubsection{Center}
We can rewrite the quadratic form with
\begin{equation}
	\vec{P} = \begin{bmatrix}
		x\\
		y
		\end{bmatrix},
		\;
		\bm{Q} = \begin{bmatrix}
		a & \dfrac{b}{2}\\
		\dfrac{b}{2} & c
		\end{bmatrix},\;
		\vec{L} = \begin{bmatrix}
		d\\
		e
		\end{bmatrix}
\end{equation}
as
\begin{equation}
	\vec{P}^t \bm{Q} \vec{P} + \vec{L}\cdot\vec{P} + f = 0.
\end{equation}
At the center $\vec{J}$ of the ellipse, the gradient of the quadratic form 
vanishes, so that formally
\begin{equation}
	\bm{Q}\vec{J} = -\dfrac{1}{2}\vec{L} \Rightarrow \vec{J} = -\dfrac{1}{2} \bm{Q}^{-1} \vec{L}.
\end{equation}
We now translate the quadratic form with
\begin{equation}
	\vec{P} = \vec{P}' + \vec{J},
\end{equation}
to find
\begin{equation}
	\vec{P}'^t \bm{Q} \vec{P}'= - \dfrac{1}{2} \vec{L}\cdot\vec{J} - f
\end{equation}

 There exist $\bm{R}\in\mathbb{O}^2$ such that
\begin{equation}
\bm{R}^tQ\bm{R} = \begin{bmatrix}
\Lambda_X & 0 \\
0         & \Lambda_Y\\
\end{bmatrix}
\end{equation}

\end{document}
